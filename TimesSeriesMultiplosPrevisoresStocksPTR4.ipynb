{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.3', '3.10.0', '2.16.1', '1.5.2')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__, matplotlib.__version__, tf.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1242 rows x 7 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base = pd.read_csv(\"C:/Users/nanojau/OneDrive/Documentos/CURSOS/DeepLearningWithPythonUdemy/petr4_treinamento.csv\")\n",
    "base = base.dropna()\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = base.iloc[:, 1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizador_previsao = MinMaxScaler(feature_range=(0,1))\n",
    "normalizador_previsao.fit_transform(base_treinamento[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241 1151\n"
     ]
    }
   ],
   "source": [
    "X = [] #previsores\n",
    "y = [] #preço real\n",
    "for i in range(90, 1242): # preço real será previsot em relação aos 90 preços anteriores\n",
    "    X.append(base_treinamento_normalizada[i - 90:i, 0:6])\n",
    "    y.append(base_treinamento_normalizada[i, 0])\n",
    "print(i, i-90)\n",
    "X, y  = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "         0.04318274],\n",
       "        [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "         0.0437121 ],\n",
       "        [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "         0.05170752],\n",
       "        [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "         0.04015963],\n",
       "        [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "         0.0416214 ],\n",
       "        [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "         0.03485382],\n",
       "        [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "         0.02507502],\n",
       "        [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "         0.0260728 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "         0.0404927 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "         0.0423977 ],\n",
       "        [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "         0.02401858],\n",
       "        [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "         0.02821315],\n",
       "        [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "         0.02706042],\n",
       "        [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "         0.02587622],\n",
       "        [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "         0.03367205],\n",
       "        [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "         0.02460946],\n",
       "        [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "         0.02806007],\n",
       "        [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "         0.02878973],\n",
       "        [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "         0.03876941],\n",
       "        [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "         0.09583767],\n",
       "        [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "         0.04756616],\n",
       "        [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "         0.04129104],\n",
       "        [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "         0.04620398],\n",
       "        [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "         0.11064144],\n",
       "        [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "         0.04418925],\n",
       "        [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "         0.0530315 ],\n",
       "        [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "         0.04444964],\n",
       "        [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "         0.03194532],\n",
       "        [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "         0.0370597 ],\n",
       "        [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "         0.0357702 ],\n",
       "        [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "         0.02903152],\n",
       "        [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "         0.0412361 ],\n",
       "        [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "         0.03711206],\n",
       "        [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "         0.04346845],\n",
       "        [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "         0.04421171],\n",
       "        [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "         0.04364257],\n",
       "        [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "         0.04779322],\n",
       "        [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "         0.04092922],\n",
       "        [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "         0.03981569],\n",
       "        [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "         0.04509603],\n",
       "        [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "         0.05085238],\n",
       "        [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "         0.04531064],\n",
       "        [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "         0.10572707],\n",
       "        [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "         0.08930445],\n",
       "        [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "         0.04376518],\n",
       "        [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "         0.03589495],\n",
       "        [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "         0.03739277],\n",
       "        [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "         0.04530406],\n",
       "        [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "         0.03887614],\n",
       "        [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "         0.06269313],\n",
       "        [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "         0.05787405],\n",
       "        [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "         0.04839097],\n",
       "        [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "         0.03954013],\n",
       "        [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "         0.03144514],\n",
       "        [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "         0.02308847],\n",
       "        [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "         0.03482392],\n",
       "        [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "         0.02257928],\n",
       "        [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "         0.01903582],\n",
       "        [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "         0.02224034],\n",
       "        [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "         0.02942397],\n",
       "        [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "         0.02244093],\n",
       "        [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "         0.02782257],\n",
       "        [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "         0.02440802],\n",
       "        [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "         0.03144357],\n",
       "        [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "         0.04400526],\n",
       "        [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "         0.04546845],\n",
       "        [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "         0.03177292],\n",
       "        [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "         0.03919891],\n",
       "        [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "         0.05120333],\n",
       "        [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "         0.03397579],\n",
       "        [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "         0.05635362],\n",
       "        [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "         0.04077971],\n",
       "        [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "         0.0548714 ],\n",
       "        [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "         0.03461346],\n",
       "        [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "         0.04969664],\n",
       "        [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "         0.04766145],\n",
       "        [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "         0.05031056],\n",
       "        [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "         0.10120858],\n",
       "        [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "         0.06567045],\n",
       "        [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "         0.04828195],\n",
       "        [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "         0.06152981],\n",
       "        [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "         0.04455508],\n",
       "        [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "         0.03775975],\n",
       "        [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "         0.03492235],\n",
       "        [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "         0.03271233],\n",
       "        [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "         0.0315204 ],\n",
       "        [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "         0.03087142],\n",
       "        [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "         0.04384244],\n",
       "        [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "         0.03128876],\n",
       "        [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "         0.03586405]]),\n",
       " 0.7611434108527131)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] , y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 6)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanojau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences=True, input_shape = (X.shape[1], 6)))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam', loss = 'mean_squared_error',metrics = ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função utilizada para interromper o treinamento caso o valor do erro não melhore após um número determinado de épocas\n",
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=10, verbose=True) #0.0000000...1\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='pesos.keras', monitor='loss', save_best_only=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0710 - mean_absolute_error: 0.1952\n",
      "Epoch 1: loss improved from inf to 0.03148, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - loss: 0.0700 - mean_absolute_error: 0.1933 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0095 - mean_absolute_error: 0.0764\n",
      "Epoch 2: loss improved from 0.03148 to 0.00963, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0095 - mean_absolute_error: 0.0764 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0081 - mean_absolute_error: 0.0700\n",
      "Epoch 3: loss improved from 0.00963 to 0.00835, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0081 - mean_absolute_error: 0.0700 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0077 - mean_absolute_error: 0.0672\n",
      "Epoch 4: loss improved from 0.00835 to 0.00689, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0077 - mean_absolute_error: 0.0671 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0079 - mean_absolute_error: 0.0692\n",
      "Epoch 5: loss did not improve from 0.00689\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0079 - mean_absolute_error: 0.0692 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0064 - mean_absolute_error: 0.0615\n",
      "Epoch 6: loss improved from 0.00689 to 0.00625, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0064 - mean_absolute_error: 0.0615 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0063 - mean_absolute_error: 0.0617\n",
      "Epoch 7: loss improved from 0.00625 to 0.00625, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.0063 - mean_absolute_error: 0.0616 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0057 - mean_absolute_error: 0.0584\n",
      "Epoch 8: loss improved from 0.00625 to 0.00609, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0057 - mean_absolute_error: 0.0585 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0051 - mean_absolute_error: 0.0559\n",
      "Epoch 9: loss improved from 0.00609 to 0.00553, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0051 - mean_absolute_error: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0053 - mean_absolute_error: 0.0555\n",
      "Epoch 10: loss did not improve from 0.00553\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0053 - mean_absolute_error: 0.0556 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0064 - mean_absolute_error: 0.0606\n",
      "Epoch 11: loss did not improve from 0.00553\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0064 - mean_absolute_error: 0.0606 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0052 - mean_absolute_error: 0.0566\n",
      "Epoch 12: loss did not improve from 0.00553\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0053 - mean_absolute_error: 0.0566 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0056 - mean_absolute_error: 0.0563\n",
      "Epoch 13: loss did not improve from 0.00553\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0056 - mean_absolute_error: 0.0564 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0059 - mean_absolute_error: 0.0588\n",
      "Epoch 14: loss improved from 0.00553 to 0.00516, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0059 - mean_absolute_error: 0.0587 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0047 - mean_absolute_error: 0.0536\n",
      "Epoch 15: loss improved from 0.00516 to 0.00469, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.0047 - mean_absolute_error: 0.0536 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0047 - mean_absolute_error: 0.0513\n",
      "Epoch 16: loss improved from 0.00469 to 0.00454, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0047 - mean_absolute_error: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0044 - mean_absolute_error: 0.0500\n",
      "Epoch 17: loss did not improve from 0.00454\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0044 - mean_absolute_error: 0.0500 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0042 - mean_absolute_error: 0.0495\n",
      "Epoch 18: loss improved from 0.00454 to 0.00426, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0042 - mean_absolute_error: 0.0495 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0039 - mean_absolute_error: 0.0478\n",
      "Epoch 19: loss improved from 0.00426 to 0.00419, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0039 - mean_absolute_error: 0.0478 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0043 - mean_absolute_error: 0.0495\n",
      "Epoch 20: loss did not improve from 0.00419\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0043 - mean_absolute_error: 0.0495 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0042 - mean_absolute_error: 0.0487\n",
      "Epoch 21: loss improved from 0.00419 to 0.00386, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0042 - mean_absolute_error: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0044 - mean_absolute_error: 0.0505\n",
      "Epoch 22: loss improved from 0.00386 to 0.00379, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0044 - mean_absolute_error: 0.0504 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0036 - mean_absolute_error: 0.0459\n",
      "Epoch 23: loss improved from 0.00379 to 0.00329, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0036 - mean_absolute_error: 0.0459 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0033 - mean_absolute_error: 0.0445\n",
      "Epoch 24: loss did not improve from 0.00329\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0033 - mean_absolute_error: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0038 - mean_absolute_error: 0.0474\n",
      "Epoch 25: loss did not improve from 0.00329\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0038 - mean_absolute_error: 0.0473 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0033 - mean_absolute_error: 0.0431\n",
      "Epoch 26: loss did not improve from 0.00329\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0033 - mean_absolute_error: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0038 - mean_absolute_error: 0.0472\n",
      "Epoch 27: loss did not improve from 0.00329\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0038 - mean_absolute_error: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0030 - mean_absolute_error: 0.0414\n",
      "Epoch 28: loss improved from 0.00329 to 0.00296, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0030 - mean_absolute_error: 0.0414 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0036 - mean_absolute_error: 0.0465\n",
      "Epoch 29: loss did not improve from 0.00296\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0036 - mean_absolute_error: 0.0465 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0033 - mean_absolute_error: 0.0440\n",
      "Epoch 30: loss did not improve from 0.00296\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0033 - mean_absolute_error: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0028 - mean_absolute_error: 0.0412\n",
      "Epoch 31: loss improved from 0.00296 to 0.00280, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0028 - mean_absolute_error: 0.0412 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0029 - mean_absolute_error: 0.0406\n",
      "Epoch 32: loss improved from 0.00280 to 0.00278, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0029 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0029 - mean_absolute_error: 0.0397\n",
      "Epoch 33: loss did not improve from 0.00278\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0029 - mean_absolute_error: 0.0397 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0032 - mean_absolute_error: 0.0428\n",
      "Epoch 34: loss did not improve from 0.00278\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0032 - mean_absolute_error: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0029 - mean_absolute_error: 0.0420\n",
      "Epoch 35: loss did not improve from 0.00278\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0029 - mean_absolute_error: 0.0420 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0029 - mean_absolute_error: 0.0406\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 36: loss did not improve from 0.00278\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0029 - mean_absolute_error: 0.0407 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0026 - mean_absolute_error: 0.0378\n",
      "Epoch 37: loss improved from 0.00278 to 0.00259, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0026 - mean_absolute_error: 0.0379 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0026 - mean_absolute_error: 0.0395\n",
      "Epoch 38: loss improved from 0.00259 to 0.00246, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0026 - mean_absolute_error: 0.0395 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0024 - mean_absolute_error: 0.0377\n",
      "Epoch 39: loss improved from 0.00246 to 0.00232, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0024 - mean_absolute_error: 0.0377 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0028 - mean_absolute_error: 0.0396\n",
      "Epoch 40: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0028 - mean_absolute_error: 0.0396 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0027 - mean_absolute_error: 0.0389\n",
      "Epoch 41: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0027 - mean_absolute_error: 0.0389 - learning_rate: 2.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0025 - mean_absolute_error: 0.0385\n",
      "Epoch 42: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0025 - mean_absolute_error: 0.0385 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0028 - mean_absolute_error: 0.0394\n",
      "Epoch 43: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0028 - mean_absolute_error: 0.0393 - learning_rate: 2.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0026 - mean_absolute_error: 0.0391\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 44: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0026 - mean_absolute_error: 0.0391 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0024 - mean_absolute_error: 0.0364\n",
      "Epoch 45: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0024 - mean_absolute_error: 0.0365 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0025 - mean_absolute_error: 0.0381\n",
      "Epoch 46: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0025 - mean_absolute_error: 0.0381 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0030 - mean_absolute_error: 0.0402\n",
      "Epoch 47: loss did not improve from 0.00232\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0030 - mean_absolute_error: 0.0401 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0024 - mean_absolute_error: 0.0361\n",
      "Epoch 48: loss improved from 0.00232 to 0.00230, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0024 - mean_absolute_error: 0.0361 - learning_rate: 4.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0026 - mean_absolute_error: 0.0379\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 49: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0026 - mean_absolute_error: 0.0379 - learning_rate: 4.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0024 - mean_absolute_error: 0.0365\n",
      "Epoch 50: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0024 - mean_absolute_error: 0.0365 - learning_rate: 8.0000e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0026 - mean_absolute_error: 0.0380\n",
      "Epoch 51: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0026 - mean_absolute_error: 0.0380 - learning_rate: 8.0000e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0022 - mean_absolute_error: 0.0359\n",
      "Epoch 52: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0022 - mean_absolute_error: 0.0359 - learning_rate: 8.0000e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0024 - mean_absolute_error: 0.0369\n",
      "Epoch 53: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0024 - mean_absolute_error: 0.0369 - learning_rate: 8.0000e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0024 - mean_absolute_error: 0.0372\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 54: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0024 - mean_absolute_error: 0.0372 - learning_rate: 8.0000e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0026 - mean_absolute_error: 0.0370\n",
      "Epoch 55: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0026 - mean_absolute_error: 0.0370 - learning_rate: 1.6000e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0023 - mean_absolute_error: 0.0366\n",
      "Epoch 56: loss did not improve from 0.00230\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0023 - mean_absolute_error: 0.0366 - learning_rate: 1.6000e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0020 - mean_absolute_error: 0.0340\n",
      "Epoch 57: loss improved from 0.00230 to 0.00226, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0020 - mean_absolute_error: 0.0340 - learning_rate: 1.6000e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0024 - mean_absolute_error: 0.0377\n",
      "Epoch 58: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0024 - mean_absolute_error: 0.0377 - learning_rate: 1.6000e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0025 - mean_absolute_error: 0.0379\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 59: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0025 - mean_absolute_error: 0.0379 - learning_rate: 1.6000e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0365\n",
      "Epoch 60: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0364 - learning_rate: 3.2000e-07\n",
      "Epoch 61/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0026 - mean_absolute_error: 0.0371\n",
      "Epoch 61: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0026 - mean_absolute_error: 0.0371 - learning_rate: 3.2000e-07\n",
      "Epoch 62/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0023 - mean_absolute_error: 0.0362\n",
      "Epoch 62: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.0023 - mean_absolute_error: 0.0362 - learning_rate: 3.2000e-07\n",
      "Epoch 63/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0026 - mean_absolute_error: 0.0376\n",
      "Epoch 63: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 0.0025 - mean_absolute_error: 0.0376 - learning_rate: 3.2000e-07\n",
      "Epoch 64/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0026 - mean_absolute_error: 0.0380 \n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "\n",
      "Epoch 64: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.0026 - mean_absolute_error: 0.0379 - learning_rate: 3.2000e-07\n",
      "Epoch 65/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0028 - mean_absolute_error: 0.0386\n",
      "Epoch 65: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.0028 - mean_absolute_error: 0.0386 - learning_rate: 6.4000e-08\n",
      "Epoch 66/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0360\n",
      "Epoch 66: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0361 - learning_rate: 6.4000e-08\n",
      "Epoch 67/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0023 - mean_absolute_error: 0.0364\n",
      "Epoch 67: loss did not improve from 0.00226\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0023 - mean_absolute_error: 0.0364 - learning_rate: 6.4000e-08\n",
      "Epoch 67: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b483d7be90>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X, y, epochs=100, batch_size=32, callbacks=[es, rlr,mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>35070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.766466</td>\n",
       "      <td>28547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>16.879999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.215569</td>\n",
       "      <td>37921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.265469</td>\n",
       "      <td>45912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.315371</td>\n",
       "      <td>28945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.840000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.614771</td>\n",
       "      <td>58618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>17.809999</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>18.323355</td>\n",
       "      <td>58488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>18.183632</td>\n",
       "      <td>48575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.223553</td>\n",
       "      <td>33470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.090000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.433134</td>\n",
       "      <td>33920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.240000</td>\n",
       "      <td>18.203592</td>\n",
       "      <td>35567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>89768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       "1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       "2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       "3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       "4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       "5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       "6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       "7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       "8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       "9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       "10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       "11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       "12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       "13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       "14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       "15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       "16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "      Volume  \n",
       "0   33461800  \n",
       "1   55940900  \n",
       "2   37064900  \n",
       "3   26958200  \n",
       "4   28400000  \n",
       "5   35070900  \n",
       "6   28547700  \n",
       "7   37921500  \n",
       "8   45912100  \n",
       "9   28945400  \n",
       "10  58618300  \n",
       "11  58488900  \n",
       "12  48575800  \n",
       "13  33470200  \n",
       "14  33920000  \n",
       "15  35567700  \n",
       "16  89768200  \n",
       "17         0  \n",
       "18  81989500  \n",
       "19  55726200  \n",
       "20  46203000  \n",
       "21  41576600  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste = pd.read_csv(\"C:/Users/nanojau/OneDrive/Documentos/CURSOS/DeepLearningWithPythonUdemy/petr4_teste.csv\")\n",
    "base_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste = base_teste.iloc[:, 1:2].values\n",
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames= [base, base_teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_completa = pd.concat(frames)\n",
    "base_completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = base_completa.drop('Date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 22, 1152)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_completa), len(base_teste), len(base_completa) - len(base_teste) - 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = base_completa[len(base_completa) - len(base_teste) -90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3930000e+01, 1.4030000e+01, 1.3760000e+01, 1.3870000e+01,\n",
       "        1.3842316e+01, 2.7208100e+07],\n",
       "       [1.3760000e+01, 1.3850000e+01, 1.3680000e+01, 1.3850000e+01,\n",
       "        1.3822356e+01, 2.7306400e+07],\n",
       "       [1.3790000e+01, 1.3900000e+01, 1.3440000e+01, 1.3450000e+01,\n",
       "        1.3423154e+01, 5.8871700e+07],\n",
       "       [1.3530000e+01, 1.3770000e+01, 1.3470000e+01, 1.3650000e+01,\n",
       "        1.3622754e+01, 8.2909400e+07],\n",
       "       [1.3850000e+01, 1.4190000e+01, 1.3820000e+01, 1.4020000e+01,\n",
       "        1.3992017e+01, 6.0260300e+07],\n",
       "       [1.3960000e+01, 1.4180000e+01, 1.3940000e+01, 1.4170000e+01,\n",
       "        1.4141717e+01, 1.8139300e+07],\n",
       "       [1.4570000e+01, 1.4650000e+01, 1.4230000e+01, 1.4410000e+01,\n",
       "        1.4381238e+01, 5.6476800e+07],\n",
       "       [1.4650000e+01, 1.5020000e+01, 1.4510000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 6.8418200e+07],\n",
       "       [1.5020000e+01, 1.5020000e+01, 1.5020000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 0.0000000e+00],\n",
       "       [1.5100000e+01, 1.5150000e+01, 1.4690000e+01, 1.4710000e+01,\n",
       "        1.4680639e+01, 3.6337400e+07],\n",
       "       [1.4880000e+01, 1.5050000e+01, 1.4810000e+01, 1.4990000e+01,\n",
       "        1.4960080e+01, 3.4915900e+07],\n",
       "       [1.4980000e+01, 1.5160000e+01, 1.4860000e+01, 1.4870000e+01,\n",
       "        1.4840320e+01, 4.9702800e+07],\n",
       "       [1.4940000e+01, 1.5100000e+01, 1.4810000e+01, 1.5030000e+01,\n",
       "        1.5000000e+01, 3.7010200e+07],\n",
       "       [1.5030000e+01, 1.5260000e+01, 1.5020000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 3.4413800e+07],\n",
       "       [1.5070000e+01, 1.5170000e+01, 1.4990000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7784700e+07],\n",
       "       [1.5020000e+01, 1.5190000e+01, 1.4980000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7601200e+07],\n",
       "       [1.5100000e+01, 1.5170000e+01, 1.4920000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 3.5822100e+07],\n",
       "       [1.5250000e+01, 1.5880000e+01, 1.5070000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 8.0267000e+07],\n",
       "       [1.5850000e+01, 1.5960000e+01, 1.5580000e+01, 1.5670000e+01,\n",
       "        1.5638723e+01, 4.6258800e+07],\n",
       "       [1.5600000e+01, 1.5800000e+01, 1.5430000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.0928300e+07],\n",
       "       [1.5790000e+01, 1.5960000e+01, 1.5700000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.6733200e+07],\n",
       "       [1.5860000e+01, 1.5900000e+01, 1.5560000e+01, 1.5560000e+01,\n",
       "        1.5528943e+01, 3.7874200e+07],\n",
       "       [1.5700000e+01, 1.5720000e+01, 1.5110000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 4.1819300e+07],\n",
       "       [1.5370000e+01, 1.5500000e+01, 1.5220000e+01, 1.5340000e+01,\n",
       "        1.5309381e+01, 3.3829000e+07],\n",
       "       [1.5500000e+01, 1.5520000e+01, 1.5300000e+01, 1.5300000e+01,\n",
       "        1.5269462e+01, 2.8638300e+07],\n",
       "       [1.5190000e+01, 1.5400000e+01, 1.5060000e+01, 1.5400000e+01,\n",
       "        1.5369262e+01, 2.9826200e+07],\n",
       "       [1.5600000e+01, 1.5980000e+01, 1.5520000e+01, 1.5980000e+01,\n",
       "        1.5948104e+01, 5.0636700e+07],\n",
       "       [1.5900000e+01, 1.5940000e+01, 1.5650000e+01, 1.5660000e+01,\n",
       "        1.5628743e+01, 4.7798600e+07],\n",
       "       [1.5880000e+01, 1.6110001e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 5.5361300e+07],\n",
       "       [1.5660000e+01, 1.5770000e+01, 1.5540000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.1741300e+07],\n",
       "       [1.5610000e+01, 1.5890000e+01, 1.5590000e+01, 1.5890000e+01,\n",
       "        1.5858284e+01, 2.7904700e+07],\n",
       "       [1.6129999e+01, 1.6190001e+01, 1.6010000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 4.7066600e+07],\n",
       "       [1.6170000e+01, 1.6250000e+01, 1.6010000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 4.0422100e+07],\n",
       "       [1.6080000e+01, 1.6080000e+01, 1.6080000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 0.0000000e+00],\n",
       "       [1.6230000e+01, 1.6290001e+01, 1.6059999e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 2.4210000e+07],\n",
       "       [1.6160000e+01, 1.6260000e+01, 1.6000000e+01, 1.6120001e+01,\n",
       "        1.6087826e+01, 4.4699700e+07],\n",
       "       [1.6139999e+01, 1.6219999e+01, 1.6070000e+01, 1.6129999e+01,\n",
       "        1.6097803e+01, 2.5524800e+07],\n",
       "       [1.6219999e+01, 1.6280001e+01, 1.6129999e+01, 1.6160000e+01,\n",
       "        1.6127745e+01, 2.5706200e+07],\n",
       "       [1.6000000e+01, 1.6160000e+01, 1.5900000e+01, 1.6150000e+01,\n",
       "        1.6117765e+01, 2.4672800e+07],\n",
       "       [1.6190001e+01, 1.6389999e+01, 1.6170000e+01, 1.6219999e+01,\n",
       "        1.6187624e+01, 3.2417500e+07],\n",
       "       [1.6290001e+01, 1.6290001e+01, 1.6120001e+01, 1.6200001e+01,\n",
       "        1.6167665e+01, 2.9389900e+07],\n",
       "       [1.6290001e+01, 1.6510000e+01, 1.6120001e+01, 1.6510000e+01,\n",
       "        1.6477047e+01, 4.6249500e+07],\n",
       "       [1.6530001e+01, 1.6730000e+01, 1.6450001e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.7608200e+07],\n",
       "       [1.6780001e+01, 1.6889999e+01, 1.6660000e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7848300e+07],\n",
       "       [1.6770000e+01, 1.7090000e+01, 1.6650000e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 4.5640100e+07],\n",
       "       [1.6969999e+01, 1.7170000e+01, 1.6740000e+01, 1.6780001e+01,\n",
       "        1.6746508e+01, 5.5355600e+07],\n",
       "       [1.6900000e+01, 1.6950001e+01, 1.6719999e+01, 1.6770000e+01,\n",
       "        1.6736528e+01, 3.2249000e+07],\n",
       "       [1.6990000e+01, 1.7100000e+01, 1.6879999e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 3.8876600e+07],\n",
       "       [1.6900000e+01, 1.6900000e+01, 1.6900000e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 0.0000000e+00],\n",
       "       [1.6959999e+01, 1.7010000e+01, 1.6680000e+01, 1.6940001e+01,\n",
       "        1.6906189e+01, 3.2605400e+07],\n",
       "       [1.7049999e+01, 1.7440001e+01, 1.6980000e+01, 1.7430000e+01,\n",
       "        1.7395210e+01, 4.6056100e+07],\n",
       "       [1.7309999e+01, 1.7350000e+01, 1.6500000e+01, 1.6500000e+01,\n",
       "        1.6467066e+01, 6.1098400e+07],\n",
       "       [1.6690001e+01, 1.6950001e+01, 1.6510000e+01, 1.6950001e+01,\n",
       "        1.6916168e+01, 4.1179600e+07],\n",
       "       [1.6889999e+01, 1.6940001e+01, 1.6719999e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 2.9399400e+07],\n",
       "       [1.6709999e+01, 1.6809999e+01, 1.6510000e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.5959400e+07],\n",
       "       [1.6690001e+01, 1.6770000e+01, 1.6389999e+01, 1.6639999e+01,\n",
       "        1.6606787e+01, 2.8697700e+07],\n",
       "       [1.6639999e+01, 1.6639999e+01, 1.5280000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 8.8765600e+07],\n",
       "       [1.5350000e+01, 1.5350000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 0.0000000e+00],\n",
       "       [1.5620000e+01, 1.6040001e+01, 1.5480000e+01, 1.5810000e+01,\n",
       "        1.5778444e+01, 4.2703800e+07],\n",
       "       [1.5920000e+01, 1.6120001e+01, 1.5810000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 3.8376900e+07],\n",
       "       [1.6020000e+01, 1.6020000e+01, 1.6020000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 0.0000000e+00],\n",
       "       [1.6150000e+01, 1.6309999e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 4.5817800e+07],\n",
       "       [1.6090000e+01, 1.6240000e+01, 1.5930000e+01, 1.6110001e+01,\n",
       "        1.6077845e+01, 3.7444900e+07],\n",
       "       [1.5980000e+01, 1.6260000e+01, 1.5940000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 1.5403600e+07],\n",
       "       [1.6250000e+01, 1.6370001e+01, 1.6040001e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.8790700e+07],\n",
       "       [1.6010000e+01, 1.6020000e+01, 1.5780000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 2.8445800e+07],\n",
       "       [1.5930000e+01, 1.6040001e+01, 1.5810000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.0429600e+07],\n",
       "       [1.5870000e+01, 1.5920000e+01, 1.5320000e+01, 1.5330000e+01,\n",
       "        1.5299401e+01, 4.5973000e+07],\n",
       "       [1.5300000e+01, 1.5470000e+01, 1.4990000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 5.2811400e+07],\n",
       "       [1.5340000e+01, 1.5770000e+01, 1.5260000e+01, 1.5610000e+01,\n",
       "        1.5578842e+01, 4.2703800e+07],\n",
       "       [1.5650000e+01, 1.5800000e+01, 1.5460000e+01, 1.5480000e+01,\n",
       "        1.5449101e+01, 4.3821500e+07],\n",
       "       [1.5500000e+01, 1.5830000e+01, 1.5210000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 3.0228000e+07],\n",
       "       [1.5220000e+01, 1.5700000e+01, 1.5140000e+01, 1.5520000e+01,\n",
       "        1.5489023e+01, 3.9238500e+07],\n",
       "       [1.5300000e+01, 1.5490000e+01, 1.5070000e+01, 1.5260000e+01,\n",
       "        1.5229542e+01, 3.7281400e+07],\n",
       "       [1.5510000e+01, 1.5680000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 3.9584500e+07],\n",
       "       [1.5480000e+01, 1.5570000e+01, 1.5370000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 2.1281600e+07],\n",
       "       [1.5360000e+01, 1.5490000e+01, 1.5180000e+01, 1.5490000e+01,\n",
       "        1.5459082e+01, 3.6201200e+07],\n",
       "       [1.5650000e+01, 1.5680000e+01, 1.5110000e+01, 1.5180000e+01,\n",
       "        1.5149701e+01, 4.6828900e+07],\n",
       "       [1.5100000e+01, 1.5310000e+01, 1.5000000e+01, 1.5010000e+01,\n",
       "        1.4980041e+01, 3.7177300e+07],\n",
       "       [1.5050000e+01, 1.5240000e+01, 1.4950000e+01, 1.4950000e+01,\n",
       "        1.4920160e+01, 5.5668300e+07],\n",
       "       [1.5160000e+01, 1.5330000e+01, 1.5130000e+01, 1.5220000e+01,\n",
       "        1.5189621e+01, 4.2760400e+07],\n",
       "       [1.5180000e+01, 1.5250000e+01, 1.5060000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 2.2639700e+07],\n",
       "       [1.5210000e+01, 1.5300000e+01, 1.5170000e+01, 1.5240000e+01,\n",
       "        1.5209581e+01, 2.0149700e+07],\n",
       "       [1.5310000e+01, 1.5870000e+01, 1.5300000e+01, 1.5860000e+01,\n",
       "        1.5828343e+01, 4.7219400e+07],\n",
       "       [1.5750000e+01, 1.5890000e+01, 1.5690000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 1.8708500e+07],\n",
       "       [1.5750000e+01, 1.5750000e+01, 1.5750000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 0.0000000e+00],\n",
       "       [1.5750000e+01, 1.5990000e+01, 1.5690000e+01, 1.5970000e+01,\n",
       "        1.5938125e+01, 2.2173100e+07],\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00],\n",
       "       [1.6190001e+01, 1.6549999e+01, 1.6190001e+01, 1.6549999e+01,\n",
       "        1.6516966e+01, 3.3461800e+07],\n",
       "       [1.6490000e+01, 1.6719999e+01, 1.6370001e+01, 1.6700001e+01,\n",
       "        1.6666668e+01, 5.5940900e+07],\n",
       "       [1.6780001e+01, 1.6959999e+01, 1.6620001e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7064900e+07],\n",
       "       [1.6700001e+01, 1.6860001e+01, 1.6570000e+01, 1.6830000e+01,\n",
       "        1.6796408e+01, 2.6958200e+07],\n",
       "       [1.6740000e+01, 1.7030001e+01, 1.6709999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 2.8400000e+07],\n",
       "       [1.7030001e+01, 1.7160000e+01, 1.6959999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 3.5070900e+07],\n",
       "       [1.6920000e+01, 1.7049999e+01, 1.6770000e+01, 1.6799999e+01,\n",
       "        1.6766466e+01, 2.8547700e+07],\n",
       "       [1.6879999e+01, 1.7299999e+01, 1.6840000e+01, 1.7250000e+01,\n",
       "        1.7215569e+01, 3.7921500e+07],\n",
       "       [1.7040001e+01, 1.7410000e+01, 1.7020000e+01, 1.7299999e+01,\n",
       "        1.7265469e+01, 4.5912100e+07],\n",
       "       [1.7320000e+01, 1.7440001e+01, 1.7150000e+01, 1.7350000e+01,\n",
       "        1.7315371e+01, 2.8945400e+07],\n",
       "       [1.7350000e+01, 1.7840000e+01, 1.7299999e+01, 1.7650000e+01,\n",
       "        1.7614771e+01, 5.8618300e+07],\n",
       "       [1.7920000e+01, 1.8360001e+01, 1.7809999e+01, 1.8360001e+01,\n",
       "        1.8323355e+01, 5.8488900e+07],\n",
       "       [1.8350000e+01, 1.8530001e+01, 1.7930000e+01, 1.8219999e+01,\n",
       "        1.8183632e+01, 4.8575800e+07],\n",
       "       [1.8309999e+01, 1.8420000e+01, 1.8030001e+01, 1.8260000e+01,\n",
       "        1.8223553e+01, 3.3470200e+07],\n",
       "       [1.8260000e+01, 1.8469999e+01, 1.8090000e+01, 1.8469999e+01,\n",
       "        1.8433134e+01, 3.3920000e+07],\n",
       "       [1.8400000e+01, 1.8459999e+01, 1.8000000e+01, 1.8240000e+01,\n",
       "        1.8203592e+01, 3.5567700e+07],\n",
       "       [1.8420000e+01, 1.9629999e+01, 1.8420000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 8.9768200e+07],\n",
       "       [1.9340000e+01, 1.9340000e+01, 1.9340000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 0.0000000e+00],\n",
       "       [1.9620001e+01, 1.9980000e+01, 1.9100000e+01, 1.9930000e+01,\n",
       "        1.9890221e+01, 8.1989500e+07],\n",
       "       [1.9670000e+01, 2.0049999e+01, 1.9570000e+01, 1.9850000e+01,\n",
       "        1.9810381e+01, 5.5726200e+07],\n",
       "       [1.9770000e+01, 1.9770000e+01, 1.9360001e+01, 1.9490000e+01,\n",
       "        1.9451097e+01, 4.6203000e+07],\n",
       "       [1.9740000e+01, 1.9930000e+01, 1.9680000e+01, 1.9700001e+01,\n",
       "        1.9660681e+01, 4.1576600e+07]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47141473, 0.47309743, 0.49334698, 0.47495091, 0.47495089,\n",
       "        0.03892707],\n",
       "       [0.46317829, 0.46437227, 0.48925281, 0.47396859, 0.47396857,\n",
       "        0.03906771],\n",
       "       [0.46463178, 0.46679593, 0.47697032, 0.45432222, 0.45432219,\n",
       "        0.0842287 ],\n",
       "       [0.45203488, 0.46049443, 0.47850563, 0.46414541, 0.46414533,\n",
       "        0.11861983],\n",
       "       [0.46753876, 0.48085313, 0.4964176 , 0.48231829, 0.48231829,\n",
       "        0.08621539],\n",
       "       [0.47286822, 0.4803684 , 0.50255885, 0.48968568, 0.48968565,\n",
       "        0.02595219],\n",
       "       [0.50242248, 0.50315075, 0.5174002 , 0.5014735 , 0.50147347,\n",
       "        0.08080228],\n",
       "       [0.50629845, 0.5210858 , 0.53172979, 0.53143421, 0.53143421,\n",
       "        0.09788703],\n",
       "       [0.52422481, 0.5210858 , 0.55783009, 0.53143421, 0.53143421,\n",
       "        0.        ],\n",
       "       [0.52810078, 0.5273873 , 0.54094166, 0.51620828, 0.51620824,\n",
       "        0.05198851],\n",
       "       [0.51744186, 0.52253999, 0.54708291, 0.52996073, 0.52996068,\n",
       "        0.04995475],\n",
       "       [0.52228682, 0.52787203, 0.54964176, 0.52406682, 0.5240668 ,\n",
       "        0.0711106 ],\n",
       "       [0.52034884, 0.52496365, 0.54708291, 0.53192537, 0.53192531,\n",
       "        0.05295109],\n",
       "       [0.5247093 , 0.53271934, 0.55783009, 0.53241653, 0.53241647,\n",
       "        0.04923638],\n",
       "       [0.52664729, 0.52835676, 0.55629478, 0.53241653, 0.53241647,\n",
       "        0.06836635],\n",
       "       [0.52422481, 0.52932622, 0.55578301, 0.53241653, 0.53241647,\n",
       "        0.06810381],\n",
       "       [0.52810078, 0.52835676, 0.55271238, 0.53732812, 0.53732809,\n",
       "        0.05125126],\n",
       "       [0.53536822, 0.56277266, 0.56038895, 0.57318274, 0.57318271,\n",
       "        0.1148393 ],\n",
       "       [0.56443798, 0.56665051, 0.58648925, 0.56335956, 0.56335952,\n",
       "        0.06618322],\n",
       "       [0.55232558, 0.55889481, 0.57881269, 0.56434187, 0.56434183,\n",
       "        0.05855678],\n",
       "       [0.56153101, 0.56665051, 0.5926305 , 0.57170926, 0.57170924,\n",
       "        0.05255479],\n",
       "       [0.56492248, 0.56374212, 0.58546571, 0.55795681, 0.55795679,\n",
       "        0.05418723],\n",
       "       [0.55717054, 0.55501697, 0.56243603, 0.54567783, 0.54567781,\n",
       "        0.05983155],\n",
       "       [0.54118217, 0.54435288, 0.56806551, 0.5471513 , 0.54715123,\n",
       "        0.0483997 ],\n",
       "       [0.54748062, 0.54532235, 0.57215967, 0.54518667, 0.54518665,\n",
       "        0.04097328],\n",
       "       [0.53246124, 0.53950557, 0.55987718, 0.55009826, 0.55009823,\n",
       "        0.04267283],\n",
       "       [0.55232558, 0.56761997, 0.58341863, 0.57858549, 0.57858544,\n",
       "        0.07244675],\n",
       "       [0.56686047, 0.56568105, 0.59007165, 0.5628684 , 0.56286836,\n",
       "        0.06838623],\n",
       "       [0.56589147, 0.57392152, 0.60030706, 0.57465622, 0.57465613,\n",
       "        0.07920631],\n",
       "       [0.55523256, 0.55744062, 0.58444217, 0.56434187, 0.56434183,\n",
       "        0.05971996],\n",
       "       [0.55281008, 0.56325739, 0.58700102, 0.57416506, 0.57416503,\n",
       "        0.03992371],\n",
       "       [0.57800383, 0.57779937, 0.60849539, 0.58889988, 0.58889984,\n",
       "        0.06733895],\n",
       "       [0.57994186, 0.58070771, 0.60849539, 0.58349708, 0.58349706,\n",
       "        0.05783256],\n",
       "       [0.5755814 , 0.57246728, 0.61207779, 0.58349708, 0.58349706,\n",
       "        0.        ],\n",
       "       [0.58284884, 0.58264668, 0.6110542 , 0.58349708, 0.58349706,\n",
       "        0.03463764],\n",
       "       [0.57945736, 0.58119244, 0.60798362, 0.58546177, 0.58546174,\n",
       "        0.06395259],\n",
       "       [0.57848832, 0.57925347, 0.61156602, 0.58595283, 0.58595275,\n",
       "        0.03651875],\n",
       "       [0.58236429, 0.58216195, 0.61463659, 0.58742635, 0.58742632,\n",
       "        0.03677828],\n",
       "       [0.57170543, 0.57634513, 0.60286592, 0.5869352 , 0.58693516,\n",
       "        0.03529978],\n",
       "       [0.5809109 , 0.58749389, 0.61668373, 0.59037326, 0.59037321,\n",
       "        0.04638024],\n",
       "       [0.58575586, 0.58264668, 0.61412492, 0.58939104, 0.58939095,\n",
       "        0.04204861],\n",
       "       [0.58575586, 0.59331071, 0.61412492, 0.60461693, 0.60461692,\n",
       "        0.06616991],\n",
       "       [0.59738377, 0.60397479, 0.63101336, 0.61493122, 0.61493117,\n",
       "        0.05380666],\n",
       "       [0.60949617, 0.61173044, 0.64176049, 0.61542243, 0.61542242,\n",
       "        0.05415018],\n",
       "       [0.60901163, 0.62142511, 0.64124872, 0.63015725, 0.63015724,\n",
       "        0.06529803],\n",
       "       [0.6187015 , 0.62530296, 0.64585466, 0.61787827, 0.61787821,\n",
       "        0.07919816],\n",
       "       [0.61531008, 0.61463892, 0.64483106, 0.61738706, 0.61738705,\n",
       "        0.04613917],\n",
       "       [0.61967054, 0.62190984, 0.6530194 , 0.62377213, 0.62377209,\n",
       "        0.05562138],\n",
       "       [0.61531008, 0.61221522, 0.65404299, 0.62377213, 0.62377209,\n",
       "        0.        ],\n",
       "       [0.61821701, 0.61754726, 0.64278403, 0.62573682, 0.62573677,\n",
       "        0.04664908],\n",
       "       [0.62257747, 0.63839074, 0.65813715, 0.64980357, 0.64980352,\n",
       "        0.06589321],\n",
       "       [0.63517437, 0.63402811, 0.63357216, 0.60412577, 0.60412571,\n",
       "        0.08741447],\n",
       "       [0.60513571, 0.61463892, 0.63408393, 0.62622798, 0.62622788,\n",
       "        0.05891632],\n",
       "       [0.61482553, 0.61415419, 0.64483106, 0.61493122, 0.61493117,\n",
       "        0.0420622 ],\n",
       "       [0.6061046 , 0.60785259, 0.63408393, 0.61493122, 0.61493117,\n",
       "        0.0514477 ],\n",
       "       [0.60513571, 0.60591372, 0.62794263, 0.61100195, 0.61100196,\n",
       "        0.04105827],\n",
       "       [0.60271313, 0.59961217, 0.57113613, 0.54764246, 0.54764244,\n",
       "        0.12699839],\n",
       "       [0.54021318, 0.53708192, 0.57471853, 0.54764246, 0.54764244,\n",
       "        0.        ],\n",
       "       [0.55329457, 0.57052841, 0.58137155, 0.57023578, 0.57023577,\n",
       "        0.06109702],\n",
       "       [0.56782946, 0.57440625, 0.59825998, 0.58055013, 0.58055012,\n",
       "        0.05490645],\n",
       "       [0.57267442, 0.56955889, 0.60900716, 0.58055013, 0.58055012,\n",
       "        0.        ],\n",
       "       [0.57897287, 0.58361604, 0.60030706, 0.57465622, 0.57465613,\n",
       "        0.06555227],\n",
       "       [0.57606589, 0.58022298, 0.60440123, 0.58497061, 0.58497053,\n",
       "        0.05357303],\n",
       "       [0.57073643, 0.58119244, 0.604913  , 0.58889988, 0.58889984,\n",
       "        0.02203818],\n",
       "       [0.58381783, 0.58652453, 0.61003076, 0.5844794 , 0.58447937,\n",
       "        0.02688416],\n",
       "       [0.57218992, 0.56955889, 0.59672467, 0.57318274, 0.57318271,\n",
       "        0.04069787],\n",
       "       [0.56831395, 0.57052841, 0.59825998, 0.57170926, 0.57170924,\n",
       "        0.04353612],\n",
       "       [0.56540698, 0.56471159, 0.57318321, 0.54666014, 0.54666008,\n",
       "        0.06577432],\n",
       "       [0.5377907 , 0.54289869, 0.55629478, 0.54911594, 0.54911591,\n",
       "        0.07555813],\n",
       "       [0.53972868, 0.55744062, 0.57011259, 0.5604126 , 0.56041253,\n",
       "        0.06109702],\n",
       "       [0.55474806, 0.55889481, 0.580348  , 0.55402753, 0.55402743,\n",
       "        0.06269613],\n",
       "       [0.54748062, 0.56034901, 0.56755374, 0.54567783, 0.54567781,\n",
       "        0.04324769],\n",
       "       [0.53391473, 0.5540475 , 0.56397134, 0.55599217, 0.55599216,\n",
       "        0.05613916],\n",
       "       [0.5377907 , 0.54386815, 0.56038895, 0.54322203, 0.54322203,\n",
       "        0.0533391 ],\n",
       "       [0.54796512, 0.55307804, 0.57471853, 0.54764246, 0.54764244,\n",
       "        0.05663419],\n",
       "       [0.54651163, 0.547746  , 0.57574207, 0.54911594, 0.54911591,\n",
       "        0.03044793],\n",
       "       [0.54069767, 0.54386815, 0.56601842, 0.55451869, 0.55451864,\n",
       "        0.05179365],\n",
       "       [0.55474806, 0.55307804, 0.56243603, 0.53929276, 0.53929272,\n",
       "        0.06699887],\n",
       "       [0.52810078, 0.535143  , 0.55680655, 0.53094305, 0.53094305,\n",
       "        0.05319017],\n",
       "       [0.52567829, 0.53174988, 0.5542477 , 0.5279961 , 0.52799606,\n",
       "        0.07964554],\n",
       "       [0.53100775, 0.53611246, 0.56345957, 0.54125739, 0.54125735,\n",
       "        0.061178  ],\n",
       "       [0.53197674, 0.53223461, 0.55987718, 0.53732812, 0.53732809,\n",
       "        0.03239099],\n",
       "       [0.53343023, 0.53465826, 0.56550665, 0.54223971, 0.54223966,\n",
       "        0.0288285 ],\n",
       "       [0.53827519, 0.56228793, 0.57215967, 0.57269158, 0.5726915 ,\n",
       "        0.06755756],\n",
       "       [0.55959302, 0.56325739, 0.59211873, 0.56728883, 0.56728878,\n",
       "        0.02676656],\n",
       "       [0.55959302, 0.55647116, 0.59518936, 0.56728883, 0.56728878,\n",
       "        0.        ],\n",
       "       [0.55959302, 0.5681047 , 0.59211873, 0.57809433, 0.57809433,\n",
       "        0.03172341],\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ],\n",
       "       [0.5809109 , 0.59524959, 0.61770732, 0.60658151, 0.6065815 ,\n",
       "        0.04787434],\n",
       "       [0.59544574, 0.60349001, 0.62691919, 0.613949  , 0.61394895,\n",
       "        0.08003555],\n",
       "       [0.60949617, 0.61512356, 0.63971346, 0.61542243, 0.61542242,\n",
       "        0.05302935],\n",
       "       [0.6056202 , 0.61027635, 0.63715455, 0.62033402, 0.62033399,\n",
       "        0.03856953],\n",
       "       [0.60755814, 0.61851677, 0.64431929, 0.63015725, 0.63015724,\n",
       "        0.04063234],\n",
       "       [0.62160858, 0.62481823, 0.65711356, 0.63015725, 0.63015724,\n",
       "        0.05017651],\n",
       "       [0.61627907, 0.61948614, 0.64738997, 0.61886049, 0.61886042,\n",
       "        0.04084366],\n",
       "       [0.61434104, 0.63160441, 0.65097236, 0.6409627 , 0.64096264,\n",
       "        0.05425491],\n",
       "       [0.62209307, 0.6369365 , 0.66018424, 0.64341845, 0.64341843,\n",
       "        0.06568719],\n",
       "       [0.63565891, 0.63839074, 0.66683726, 0.64587429, 0.64587431,\n",
       "        0.04141265],\n",
       "       [0.6371124 , 0.65777993, 0.67451377, 0.66060907, 0.66060903,\n",
       "        0.08386615],\n",
       "       [0.66472868, 0.68298599, 0.70061407, 0.69548142, 0.69548138,\n",
       "        0.08368102],\n",
       "       [0.68556202, 0.69122642, 0.70675537, 0.68860509, 0.68860504,\n",
       "        0.06949819],\n",
       "       [0.68362398, 0.68589433, 0.71187313, 0.69056978, 0.69056971,\n",
       "        0.04788636],\n",
       "       [0.68120155, 0.68831794, 0.71494371, 0.70088407, 0.70088406,\n",
       "        0.0485299 ],\n",
       "       [0.6879845 , 0.6878332 , 0.71033777, 0.68958746, 0.68958735,\n",
       "        0.05088729],\n",
       "       [0.68895349, 0.74454673, 0.73183214, 0.74361497, 0.74361488,\n",
       "        0.12843282],\n",
       "       [0.73352713, 0.73048958, 0.77891505, 0.74361497, 0.74361488,\n",
       "        0.        ],\n",
       "       [0.74709307, 0.76151236, 0.76663255, 0.77259336, 0.77259335,\n",
       "        0.11730371],\n",
       "       [0.7495155 , 0.76490543, 0.79068577, 0.76866408, 0.76866409,\n",
       "        0.07972838],\n",
       "       [0.75436047, 0.75133301, 0.77993864, 0.75098236, 0.75098224,\n",
       "        0.06610338],\n",
       "       [0.75290698, 0.75908871, 0.79631525, 0.76129675, 0.76129674,\n",
       "        0.05948432]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas = normalizador.transform(entradas)\n",
    "entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = []\n",
    "for i in range(90, 112):\n",
    "    X_teste.append(entradas[i-90:i, 0:6])\n",
    "X_teste = np.array(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519ms/step\n"
     ]
    }
   ],
   "source": [
    "previsoes = regressor.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00845068],\n",
       "       [0.0085773 ],\n",
       "       [0.00874531],\n",
       "       [0.00895908],\n",
       "       [0.00921361],\n",
       "       [0.00950172],\n",
       "       [0.00981847],\n",
       "       [0.01015174],\n",
       "       [0.0104936 ],\n",
       "       [0.01084103],\n",
       "       [0.01118687],\n",
       "       [0.01153601],\n",
       "       [0.01190127],\n",
       "       [0.01228931],\n",
       "       [0.01269724],\n",
       "       [0.01311891],\n",
       "       [0.0135432 ],\n",
       "       [0.01398324],\n",
       "       [0.01443036],\n",
       "       [0.01490263],\n",
       "       [0.01540716],\n",
       "       [0.01593599]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = normalizador_previsao.inverse_transform(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.374422 ],\n",
       "       [4.377035 ],\n",
       "       [4.380503 ],\n",
       "       [4.3849154],\n",
       "       [4.3901687],\n",
       "       [4.3961153],\n",
       "       [4.402653 ],\n",
       "       [4.409532 ],\n",
       "       [4.416588 ],\n",
       "       [4.423759 ],\n",
       "       [4.430897 ],\n",
       "       [4.438103 ],\n",
       "       [4.445642 ],\n",
       "       [4.4536514],\n",
       "       [4.462071 ],\n",
       "       [4.4707747],\n",
       "       [4.479532 ],\n",
       "       [4.488614 ],\n",
       "       [4.497843 ],\n",
       "       [4.5075903],\n",
       "       [4.5180035],\n",
       "       [4.5289187]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4398785"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.87454563636364"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.434666890851107"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_teste, color = \"red\", label = \"Preço real\")\n",
    "plt.plot(previsoes, color = \"blue\", label = \"Previsões\")\n",
    "plt.title(\"Previsão do preço das ações\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
