{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.3', '3.10.0', '2.16.1', '1.5.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__, matplotlib.__version__, tf.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1242 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base = pd.read_csv(\"C:/Users/nanojau/OneDrive/Documentos/CURSOS/DeepLearningWithPythonUdemy/petr4_treinamento.csv\")\n",
    "base = base.dropna()\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = base.iloc[:, 1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241 1151\n"
     ]
    }
   ],
   "source": [
    "X = [] #previsores\n",
    "y = [] #preço real\n",
    "for i in range(90, 1242): # preço real será previsot em relação aos 90 preços anteriores\n",
    "    X.append(base_treinamento_normalizada[i - 90:i, 0:6])\n",
    "    y.append(base_treinamento_normalizada[i, 0])\n",
    "print(i, i-90)\n",
    "X, y  = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "        0.04015963],\n",
       "       [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "        0.0416214 ],\n",
       "       [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "        0.03485382],\n",
       "       [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "        0.02507502],\n",
       "       [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "        0.0260728 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "        0.0404927 ],\n",
       "       [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "        0.0423977 ],\n",
       "       [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "        0.02401858],\n",
       "       [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "        0.02821315],\n",
       "       [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "        0.02706042],\n",
       "       [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "        0.02587622],\n",
       "       [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "        0.03367205],\n",
       "       [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "        0.02460946],\n",
       "       [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "        0.02806007],\n",
       "       [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "        0.02878973],\n",
       "       [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "        0.03876941],\n",
       "       [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "        0.09583767],\n",
       "       [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "        0.04756616],\n",
       "       [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "        0.04129104],\n",
       "       [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "        0.04620398],\n",
       "       [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "        0.11064144],\n",
       "       [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "        0.04418925],\n",
       "       [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "        0.0530315 ],\n",
       "       [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "        0.04444964],\n",
       "       [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "        0.03194532],\n",
       "       [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "        0.0370597 ],\n",
       "       [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "        0.0357702 ],\n",
       "       [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "        0.02903152],\n",
       "       [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "        0.0412361 ],\n",
       "       [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "        0.03711206],\n",
       "       [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "        0.04346845],\n",
       "       [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "        0.04421171],\n",
       "       [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "        0.04364257],\n",
       "       [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "        0.04779322],\n",
       "       [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "        0.04092922],\n",
       "       [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "        0.03981569],\n",
       "       [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "        0.04509603],\n",
       "       [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "        0.05085238],\n",
       "       [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "        0.04531064],\n",
       "       [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "        0.10572707],\n",
       "       [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "        0.08930445],\n",
       "       [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "        0.04376518],\n",
       "       [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "        0.03589495],\n",
       "       [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "        0.03739277],\n",
       "       [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "        0.04530406],\n",
       "       [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "        0.03887614],\n",
       "       [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "        0.06269313],\n",
       "       [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "        0.05787405],\n",
       "       [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "        0.04839097],\n",
       "       [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "        0.03954013],\n",
       "       [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "        0.03144514],\n",
       "       [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "        0.02308847],\n",
       "       [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "        0.03482392],\n",
       "       [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "        0.02257928],\n",
       "       [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "        0.01903582],\n",
       "       [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "        0.02224034],\n",
       "       [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "        0.02942397],\n",
       "       [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "        0.02244093],\n",
       "       [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "        0.02782257],\n",
       "       [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "        0.02440802],\n",
       "       [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "        0.03144357],\n",
       "       [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "        0.04400526],\n",
       "       [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "        0.04546845],\n",
       "       [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "        0.03177292],\n",
       "       [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "        0.03919891],\n",
       "       [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "        0.05120333],\n",
       "       [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "        0.03397579],\n",
       "       [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "        0.05635362],\n",
       "       [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "        0.04077971],\n",
       "       [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "        0.0548714 ],\n",
       "       [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "        0.03461346],\n",
       "       [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "        0.04969664],\n",
       "       [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "        0.04766145],\n",
       "       [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "        0.05031056],\n",
       "       [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "        0.10120858],\n",
       "       [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "        0.06567045],\n",
       "       [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "        0.04828195],\n",
       "       [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "        0.06152981],\n",
       "       [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "        0.04455508],\n",
       "       [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "        0.03775975],\n",
       "       [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "        0.03492235],\n",
       "       [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "        0.03271233],\n",
       "       [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "        0.0315204 ],\n",
       "       [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "        0.03087142],\n",
       "       [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "        0.04384244],\n",
       "       [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "        0.03128876],\n",
       "       [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "        0.03586405]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanojau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences=True, input_shape = (X.shape[1], 6)))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam', loss = 'mean_squared_error',metrics = ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função utilizada para interromper o treinamento caso o valor do erro não melhore após um número determinado de épocas\n",
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=10, verbose=True) #0.0000000...1\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='pesos.keras', monitor='loss', save_best_only=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0474 - mean_absolute_error: 0.1577\n",
      "Epoch 1: loss improved from inf to 0.02298, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - loss: 0.0467 - mean_absolute_error: 0.1564 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0104 - mean_absolute_error: 0.0785\n",
      "Epoch 2: loss improved from 0.02298 to 0.00955, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: 0.0104 - mean_absolute_error: 0.0784 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0082 - mean_absolute_error: 0.0686\n",
      "Epoch 3: loss improved from 0.00955 to 0.00803, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.0082 - mean_absolute_error: 0.0686 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0080 - mean_absolute_error: 0.0687\n",
      "Epoch 4: loss improved from 0.00803 to 0.00797, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.0080 - mean_absolute_error: 0.0687 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0069 - mean_absolute_error: 0.0645\n",
      "Epoch 5: loss improved from 0.00797 to 0.00660, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 0.0069 - mean_absolute_error: 0.0644 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0066 - mean_absolute_error: 0.0618\n",
      "Epoch 6: loss improved from 0.00660 to 0.00657, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 0.0066 - mean_absolute_error: 0.0617 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0062 - mean_absolute_error: 0.0595\n",
      "Epoch 7: loss improved from 0.00657 to 0.00619, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0062 - mean_absolute_error: 0.0595 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0057 - mean_absolute_error: 0.0575\n",
      "Epoch 8: loss improved from 0.00619 to 0.00578, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 0.0057 - mean_absolute_error: 0.0575 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0061 - mean_absolute_error: 0.0591\n",
      "Epoch 9: loss did not improve from 0.00578\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0061 - mean_absolute_error: 0.0591 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0048 - mean_absolute_error: 0.0533\n",
      "Epoch 10: loss improved from 0.00578 to 0.00526, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0048 - mean_absolute_error: 0.0533 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0058 - mean_absolute_error: 0.0583\n",
      "Epoch 11: loss did not improve from 0.00526\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.0058 - mean_absolute_error: 0.0582 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0057 - mean_absolute_error: 0.0578\n",
      "Epoch 12: loss improved from 0.00526 to 0.00515, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0057 - mean_absolute_error: 0.0578 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0048 - mean_absolute_error: 0.0523\n",
      "Epoch 13: loss improved from 0.00515 to 0.00504, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0048 - mean_absolute_error: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0045 - mean_absolute_error: 0.0515\n",
      "Epoch 14: loss improved from 0.00504 to 0.00462, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0045 - mean_absolute_error: 0.0515 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0047 - mean_absolute_error: 0.0512\n",
      "Epoch 15: loss improved from 0.00462 to 0.00455, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0047 - mean_absolute_error: 0.0512 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0043 - mean_absolute_error: 0.0506\n",
      "Epoch 16: loss improved from 0.00455 to 0.00450, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0043 - mean_absolute_error: 0.0507 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0045 - mean_absolute_error: 0.0506\n",
      "Epoch 17: loss improved from 0.00450 to 0.00424, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0045 - mean_absolute_error: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0040 - mean_absolute_error: 0.0480\n",
      "Epoch 18: loss improved from 0.00424 to 0.00395, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 0.0040 - mean_absolute_error: 0.0480 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0043 - mean_absolute_error: 0.0494\n",
      "Epoch 19: loss did not improve from 0.00395\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0043 - mean_absolute_error: 0.0494 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0038 - mean_absolute_error: 0.0464\n",
      "Epoch 20: loss improved from 0.00395 to 0.00363, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0038 - mean_absolute_error: 0.0463 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0040 - mean_absolute_error: 0.0474\n",
      "Epoch 21: loss did not improve from 0.00363\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 0.0040 - mean_absolute_error: 0.0473 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0038 - mean_absolute_error: 0.0467\n",
      "Epoch 22: loss did not improve from 0.00363\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0038 - mean_absolute_error: 0.0467 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0034 - mean_absolute_error: 0.0445\n",
      "Epoch 23: loss improved from 0.00363 to 0.00355, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0034 - mean_absolute_error: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0036 - mean_absolute_error: 0.0460\n",
      "Epoch 24: loss did not improve from 0.00355\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.0036 - mean_absolute_error: 0.0461 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0039 - mean_absolute_error: 0.0471\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 25: loss did not improve from 0.00355\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0039 - mean_absolute_error: 0.0471 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0034 - mean_absolute_error: 0.0444\n",
      "Epoch 26: loss improved from 0.00355 to 0.00319, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.0034 - mean_absolute_error: 0.0444 - learning_rate: 2.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0026 - mean_absolute_error: 0.0403\n",
      "Epoch 27: loss improved from 0.00319 to 0.00286, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0026 - mean_absolute_error: 0.0403 - learning_rate: 2.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0030 - mean_absolute_error: 0.0417\n",
      "Epoch 28: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0030 - mean_absolute_error: 0.0417 - learning_rate: 2.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0029 - mean_absolute_error: 0.0407\n",
      "Epoch 29: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0029 - mean_absolute_error: 0.0407 - learning_rate: 2.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0030 - mean_absolute_error: 0.0413\n",
      "Epoch 30: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0030 - mean_absolute_error: 0.0414 - learning_rate: 2.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.0030 - mean_absolute_error: 0.0424\n",
      "Epoch 31: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.0030 - mean_absolute_error: 0.0424 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.0031 - mean_absolute_error: 0.0421\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 32: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.0031 - mean_absolute_error: 0.0421 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0029 - mean_absolute_error: 0.0424\n",
      "Epoch 33: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0029 - mean_absolute_error: 0.0424 - learning_rate: 4.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0034 - mean_absolute_error: 0.0440\n",
      "Epoch 34: loss did not improve from 0.00286\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.0034 - mean_absolute_error: 0.0439 - learning_rate: 4.0000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0025 - mean_absolute_error: 0.0388\n",
      "Epoch 35: loss improved from 0.00286 to 0.00269, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0025 - mean_absolute_error: 0.0388 - learning_rate: 4.0000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0029 - mean_absolute_error: 0.0421\n",
      "Epoch 36: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0029 - mean_absolute_error: 0.0420 - learning_rate: 4.0000e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0030 - mean_absolute_error: 0.0417\n",
      "Epoch 37: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 0.0030 - mean_absolute_error: 0.0417 - learning_rate: 4.0000e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.0030 - mean_absolute_error: 0.0413\n",
      "Epoch 38: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.0030 - mean_absolute_error: 0.0413 - learning_rate: 4.0000e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0028 - mean_absolute_error: 0.0402\n",
      "Epoch 39: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.0028 - mean_absolute_error: 0.0402 - learning_rate: 4.0000e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0030 - mean_absolute_error: 0.0411\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 40: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 0.0030 - mean_absolute_error: 0.0411 - learning_rate: 4.0000e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0028 - mean_absolute_error: 0.0403\n",
      "Epoch 41: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0028 - mean_absolute_error: 0.0403 - learning_rate: 8.0000e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0026 - mean_absolute_error: 0.0386\n",
      "Epoch 42: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.0026 - mean_absolute_error: 0.0387 - learning_rate: 8.0000e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0031 - mean_absolute_error: 0.0410\n",
      "Epoch 43: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0031 - mean_absolute_error: 0.0410 - learning_rate: 8.0000e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0032 - mean_absolute_error: 0.0424\n",
      "Epoch 44: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0032 - mean_absolute_error: 0.0424 - learning_rate: 8.0000e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0032 - mean_absolute_error: 0.0430\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 45: loss did not improve from 0.00269\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0032 - mean_absolute_error: 0.0430 - learning_rate: 8.0000e-06\n",
      "Epoch 45: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15d3e6e11c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X, y, epochs=100, batch_size=32, callbacks=[es, rlr,mcp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
